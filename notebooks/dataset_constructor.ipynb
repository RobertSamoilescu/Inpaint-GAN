{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "%matplotlib inline\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from util.depth_map_utils import *\n",
    "from util.generator import random_walk\n",
    "from util.inverse_warp import *\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle as pkl\n",
    "from pyquaternion import Quaternion\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 1.1 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='../v1.0-mini', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_extrinsic(cam):\n",
    "    calib_sensor = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    translation = calib_sensor['translation']\n",
    "    rotation = calib_sensor['rotation']\n",
    "\n",
    "    q = Quaternion(rotation)\n",
    "    R = q.rotation_matrix\n",
    "    T = np.array(translation).reshape(-1, 1)\n",
    "\n",
    "    # rotation to transform car's coordinate system to\n",
    "    # camera coordinate system\n",
    "    alphay = np.pi / 2\n",
    "    alphaz = -np.pi / 2\n",
    "    \n",
    "    Ry = np.array([\n",
    "        [np.cos(alphay), 0, -np.sin(alphay)],\n",
    "        [0, 1, 0],\n",
    "        [np.sin(alphay), 0, np.cos(alphay)]\n",
    "    ])\n",
    "\n",
    "    Rz = np.array([\n",
    "        [np.cos(alphaz), np.sin(alphaz), 0],\n",
    "        [-np.sin(alphaz), np.cos(alphaz), 0],\n",
    "        [0, 0, 1],\n",
    "    ])\n",
    "\n",
    "    Rot = Rz @ Ry\n",
    "    R = Rot @ R\n",
    "    T = Rot @ T\n",
    "\n",
    "    camera_extrinsic = np.hstack((R, T))\n",
    "    return camera_extrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_map(sample, cam, img):\n",
    "    # depth map\n",
    "    lidar = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "    points, coloring, im = nusc.explorer.map_pointcloud_to_image(\n",
    "        pointsensor_token=lidar['token'],\n",
    "        camera_token=cam['token']\n",
    "    )\n",
    "\n",
    "    depth_map = np.zeros((img.shape[0], img.shape[1])).astype(np.float32)\n",
    "    for i, (x, y) in enumerate(zip(points[0], points[1])):\n",
    "        int_x, int_y = np.floor([x, y]).astype(np.int)\n",
    "        int_x = np.clip(int_x, 0, depth_map.shape[1] - 1)\n",
    "        int_y = np.clip(int_y, 0, depth_map.shape[0] - 1)\n",
    "        depth_map[int_y, int_x] = coloring[i]\n",
    "    \n",
    "    # lidar preprocessing\n",
    "    extrapolate = False\n",
    "    blur_type = 'bilateral'\n",
    "\n",
    "    depth_map = fill_in_fast(\n",
    "        depth_map,\n",
    "        extrapolate=extrapolate, \n",
    "        blur_type=blur_type\n",
    "    )\n",
    "\n",
    "    depth_map, process_dict = fill_in_multiscale(\n",
    "        depth_map, \n",
    "        extrapolate=extrapolate, \n",
    "        blur_type=blur_type,\n",
    "        show_process=False\n",
    "    )\n",
    "\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transformation(imgs, intrinsics, extrinsics, depths):\n",
    "    \"\"\"\n",
    "    Generate random transformation\n",
    "    @param imgs:         [B, 3, H, W]\n",
    "    @param depths:       [B, H, W]\n",
    "    @param intrinsics:   [B, 3, 3]\n",
    "    @param extriniscs:   [B, 3, 4]\n",
    "    \"\"\"\n",
    "    # sample random transformation\n",
    "    B = imgs.shape[0]\n",
    "    poses = torch.zeros(B, 6).double()\n",
    "    tx = .75 * 2 * (torch.rand(B) - 0.5)\n",
    "    ry = 0.2 * 2 * (torch.rand(B) - 0.5)\n",
    "    poses[:, 0], poses[:, 4] = tx, ry\n",
    "    \n",
    "    # apply transformation - faster inverse-warp\n",
    "    projected_imgs, valid_points = inverse_warp(\n",
    "        img=imgs, \n",
    "        depth=depths, \n",
    "        pose=poses, \n",
    "        intrinsics=intrinsics,\n",
    "        extrinsics=None\n",
    "    )\n",
    "    \n",
    "    # mask of valid points\n",
    "    # valid_points = (valid_points * (depths > 0).type(torch.long)).double()\n",
    "    valid_points = (valid_points.type(torch.long) * (depths > 0).type(torch.long)).double()\n",
    "    projected_imgs = projected_imgs * valid_points.unsqueeze(1)\n",
    "    return projected_imgs, valid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scene(scene, idx):\n",
    "    current_sample_token = scene['first_sample_token']\n",
    "    last_sample_token = scene['last_sample_token']\n",
    "    name = scene['name']\n",
    "    \n",
    "    for i in count():\n",
    "        # load current sample\n",
    "        current_sample = nusc.get('sample', current_sample_token)\n",
    "       \n",
    "        # get cameras\n",
    "        cam_front = nusc.get('sample_data', current_sample['data']['CAM_FRONT'])\n",
    "        cam_front_right = nusc.get('sample_data', current_sample['data']['CAM_FRONT_RIGHT'])\n",
    "        cam_front_left = nusc.get('sample_data', current_sample['data']['CAM_FRONT_LEFT'])\n",
    "        cam_back = nusc.get('sample_data', current_sample['data']['CAM_BACK'])\n",
    "        cam_back_right = nusc.get('sample_data', current_sample['data']['CAM_BACK_RIGHT'])\n",
    "        cam_back_left = nusc.get('sample_data', current_sample['data']['CAM_BACK_LEFT'])\n",
    "        \n",
    "        # get images path and intrinsic matrix of central camera\n",
    "        img_front_path, _, cam_front_intrinsic = \\\n",
    "            nusc.explorer.nusc.get_sample_data(cam_front['token'])\n",
    "        img_front_right_path, _, cam_front_right_intrinsic = \\\n",
    "            nusc.explorer.nusc.get_sample_data(cam_front_right['token'])\n",
    "        img_front_left_path, _, cam_front_left_intrinsic = \\\n",
    "            nusc.explorer.nusc.get_sample_data(cam_front_left['token'])\n",
    "        img_back_path, _, cam_back_intrinsic = \\\n",
    "            nusc.explorer.nusc.get_sample_data(cam_back['token'])\n",
    "        img_back_right_path, _, cam_back_right_intrinsic = \\\n",
    "            nusc.explorer.nusc.get_sample_data(cam_back_right['token'])\n",
    "        img_back_left_path, _, cam_back_left_intrinsic = \\\n",
    "            nusc.explorer.nusc.get_sample_data(cam_back_left['token'])\n",
    "\n",
    "        # get images\n",
    "        img_front = np.array(Image.open(img_front_path))\n",
    "        img_front_right = np.array(Image.open(img_front_right_path))\n",
    "        img_front_left = np.array(Image.open(img_front_left_path))\n",
    "        img_back = np.array(Image.open(img_back_path))\n",
    "        img_back_right = np.array(Image.open(img_back_right_path))\n",
    "        img_back_left = np.array(Image.open(img_back_left_path))\n",
    "        \n",
    "        # save original dimensions\n",
    "        orig_height, orig_width, _ = img_front.shape\n",
    "        \n",
    "        # get front camera extrinsic\n",
    "        cam_front_extrinsic = get_camera_extrinsic(cam_front)\n",
    "        cam_front_right_extrinsic = get_camera_extrinsic(cam_front_right)\n",
    "        cam_front_left_extrinisc = get_camera_extrinsic(cam_front_left)\n",
    "        cam_back_extrinsic = get_camera_extrinsic(cam_back)\n",
    "        cam_back_right_extrinsic = get_camera_extrinsic(cam_back_right)\n",
    "        cam_back_left_extrinsic = get_camera_extrinsic(cam_back_left)\n",
    "\n",
    "        # get depth map\n",
    "        depth_map_front = get_depth_map(current_sample, cam_front, img_front)\n",
    "        depth_map_front_right = get_depth_map(current_sample, cam_front_right, img_front_right)\n",
    "        depth_map_front_left = get_depth_map(current_sample, cam_front_left, img_front_left)\n",
    "        depth_map_back = get_depth_map(current_sample, cam_back, img_back)\n",
    "        depth_map_back_right = get_depth_map(current_sample, cam_back_right, img_back_right)\n",
    "        depth_map_back_left = get_depth_map(current_sample, cam_back_left, img_back_left)\n",
    "            \n",
    "        # resize image and depth_map\n",
    "        height, width = 128, 256\n",
    "        \n",
    "        imgs = [\n",
    "            cv2.resize(img_front, (width, height)),\n",
    "            cv2.resize(img_front_right, (width, height)),\n",
    "            cv2.resize(img_front_left, (width, height)),\n",
    "            cv2.resize(img_back, (width, height)),\n",
    "            cv2.resize(img_back_right, (width, height)),\n",
    "            cv2.resize(img_back_left, (width, height)),            \n",
    "        ]\n",
    "        \n",
    "        depths = [\n",
    "            cv2.resize(depth_map_front, (width, height)),\n",
    "            cv2.resize(depth_map_front_right, (width, height)),\n",
    "            cv2.resize(depth_map_front_left, (width, height)),\n",
    "            cv2.resize(depth_map_back, (width, height)),\n",
    "            cv2.resize(depth_map_back_right, (width, height)),\n",
    "            cv2.resize(depth_map_back_left, (width, height)),\n",
    "        ]\n",
    "        \n",
    "        # update camera intrinsics according to the new size\n",
    "        S = np.array([\n",
    "            [width/orig_width, 0, 0],\n",
    "            [0, height/orig_height, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        intrinsics = [\n",
    "            S @ cam_front_intrinsic,\n",
    "            S @ cam_front_right_intrinsic,\n",
    "            S @ cam_front_left_intrinsic,\n",
    "            S @ cam_back_intrinsic,\n",
    "            S @ cam_back_right_intrinsic,\n",
    "            S @ cam_back_left_intrinsic,\n",
    "        ]\n",
    "        \n",
    "        extrinsics = [\n",
    "            cam_front_extrinsic,\n",
    "            cam_front_right_extrinsic,\n",
    "            cam_front_left_extrinisc,\n",
    "            cam_back_extrinsic,\n",
    "            cam_back_right_extrinsic,\n",
    "            cam_back_left_extrinsic,\n",
    "        ]\n",
    "        \n",
    "        masks = []\n",
    "        for j in range(len(imgs)):\n",
    "            _, mask = random_transformation(\n",
    "                imgs=torch.tensor(imgs[j].transpose(2, 0, 1)).unsqueeze(0).double(), \n",
    "                intrinsics=torch.tensor(intrinsics[j]).unsqueeze(0).double(), \n",
    "                extrinsics=torch.tensor(extrinsics[j]).unsqueeze(0).double(), \n",
    "                depths=torch.tensor(depths[j]).unsqueeze(0).double()\n",
    "            )\n",
    "            mask = 255 * mask.to(torch.uint8).squeeze(0).numpy()\n",
    "            masks.append(mask)\n",
    "                \n",
    "                \n",
    "        # save data to files\n",
    "        for j in range(len(imgs)):\n",
    "            cv2.imwrite(\"../dataset/imgs/%s.%d.%d.%d.png\" % (name, idx, i, j), imgs[j][:, :, ::-1])\n",
    "            cv2.imwrite(\"../dataset/masks/%s.%d.%d.%d.png\" % (name, idx, i, j), masks[j])\n",
    "            \n",
    "            with open(\"../dataset/depths/%s.%d.%d.%d.pkl\" % (name, idx, i, j), \"wb\") as f:\n",
    "                pkl.dump(depths[j], f)\n",
    "            with open(\"../dataset/intrinsics/%s.%d.%d.%d.pkl\" % (name, idx, i, j), \"wb\") as f:\n",
    "                pkl.dump(intrinsics[j], f)\n",
    "            with open(\"../dataset/extrinsics/%s.%d.%d.%d.pkl\" % (name, idx, i, j), \"wb\") as f:\n",
    "                pkl.dump(extrinsics[j], f)\n",
    "                \n",
    "        # check if last sample was processed\n",
    "        if current_sample_token == last_sample_token:\n",
    "            break\n",
    "            \n",
    "        # go to the next sample\n",
    "        current_sample_token = current_sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = Pool(processes=cpu_count() - 2)\n",
    "\n",
    "for idx, scene in enumerate(nusc.scene):\n",
    "    # process_scene(scene, idx)\n",
    "    pool.apply_async(process_scene, (scene, idx))\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
